{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def KnuthMorrisPratt(text, pattern):\n",
    "\n",
    "    '''Yields all starting positions of copies of the pattern in the text.\n",
    "Calling conventions are similar to string.find, but its arguments can be\n",
    "lists or iterators, not just strings, it returns all matches, not just\n",
    "the first one, and it does not need the whole text in memory at once.\n",
    "Whenever it yields, it will have read the text exactly up to and including\n",
    "the match that caused the yield.'''\n",
    "\n",
    "    # allow indexing into pattern and protect against change during yield\n",
    "    pattern = list(pattern)\n",
    "\n",
    "    # build table of shift amounts\n",
    "    shifts = [1] * (len(pattern) + 1)\n",
    "    shift = 1\n",
    "    for pos in range(len(pattern)):\n",
    "        while shift <= pos and pattern[pos] != pattern[pos-shift]:\n",
    "            shift += shifts[pos-shift]\n",
    "        shifts[pos+1] = shift\n",
    "\n",
    "    print(shifts)\n",
    "    # do the actual search\n",
    "    startPos = 0\n",
    "    matchLen = 0\n",
    "    for c in text:\n",
    "        while matchLen == len(pattern) or \\\n",
    "              matchLen >= 0 and pattern[matchLen] != c:\n",
    "            startPos += shifts[matchLen]\n",
    "            matchLen -= shifts[matchLen]\n",
    "        matchLen += 1\n",
    "        if matchLen == len(pattern):\n",
    "            yield startPos\n",
    "\n",
    "def search_list(text, pattern):\n",
    "    matchLen = len(pattern)\n",
    "    for i in range(len(text) - matchLen + 1):\n",
    "        startPos = 0\n",
    "        while(startPos < matchLen and text[i + startPos] == pattern[startPos]):\n",
    "            startPos += 1\n",
    "        #if (text[i:i+matchLen] == pattern): return i\n",
    "        if (startPos == matchLen): return i\n",
    "    \n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_dict = {}\n",
    "vocab_list = []\n",
    "review_lines = []\n",
    "\n",
    "i = 0\n",
    "#with open('reviews_test.txt') as f:\n",
    "with open('reviews_sample.txt') as f: \n",
    "    for line in f.read().splitlines():\n",
    "        vocab_index = \"\"\n",
    "        for word in line.split(' '):\n",
    "            if word not in vocab_dict:                \n",
    "                vocab_dict[word] = i\n",
    "                word_index = i\n",
    "                vocab_list.append(word)\n",
    "                i += 1\n",
    "            else:\n",
    "                word_index = vocab_dict[word]\n",
    "            \n",
    "            vocab_index += \":\" + str(word_index)\n",
    "        \n",
    "        vocab_index += \":\"\n",
    "        review_lines.append(vocab_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22104/22104 [02:32<00:00, 145.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "MIN_SUPPORT = len(review_lines) * 0.01\n",
    "\n",
    "def get_support(all_reviews, pattern):\n",
    "    str_pattern = \"\"\n",
    "    for i in pattern:\n",
    "        str_pattern += \":\" + str(i)\n",
    "    str_pattern += \":\"\n",
    "    \n",
    "    support = 0\n",
    "    \n",
    "    for review in all_reviews:\n",
    "        #if search_list(review, pattern) >= 0:\n",
    "        if review.find(str_pattern) >= 0:\n",
    "            support += 1\n",
    "    \n",
    "    return support\n",
    "\n",
    "L = []\n",
    "S = []\n",
    "L1 = []\n",
    "S1 = []\n",
    "for vocab in tqdm(range(len(vocab_list))):\n",
    "    s = get_support(review_lines, [vocab])    \n",
    "    if(s >= MIN_SUPPORT):\n",
    "        L1.append([vocab])\n",
    "        S1.append(s)\n",
    "\n",
    "L.append(L1)\n",
    "S.append(S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "977"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def apriori_gen(all_lines, LK_1, min_support):\n",
    "    LK = []\n",
    "    S = []\n",
    "    for l1 in tqdm(LK_1):\n",
    "        for l2 in LK_1:\n",
    "            if l1[:-1] == l2[:-1]: #and l1[-1] < l2[-1]:\n",
    "                l = l1 + [l2[-1]]\n",
    "                support = get_support(all_lines, l)\n",
    "                if support >= min_support:\n",
    "                    #print(l)\n",
    "                    #print(support)\n",
    "                    #print([vocab_list[i] for i in l])\n",
    "                    LK.append(l)\n",
    "                    S.append(support)\n",
    "    \n",
    "    return LK, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/977 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 1-Itemsets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 977/977 [1:44:34<00:00,  6.29s/it]\n",
      "  8%|▊         | 5/63 [00:00<00:01, 49.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2-Itemsets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:00<00:00, 94.87it/s]\n"
     ]
    }
   ],
   "source": [
    "LK_1 = L1\n",
    "while len(LK_1) > 0:\n",
    "    print(\"Working on %d-Itemsets\" % len(LK_1[0]))\n",
    "    LK_1, SK_1 = apriori_gen(review_lines, LK_1, MIN_SUPPORT)\n",
    "    if (len(LK_1) > 0):\n",
    "        L.append(LK_1)\n",
    "        S.append(SK_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(L[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('patterns.txt', 'w') as f:\n",
    "    for i in range(len(L)):\n",
    "        for j in range(len(L[i])):\n",
    "            f.write(\"%d:\"%(S[i][j]))\n",
    "            for k in range(len(L[i][j]) - 1):\n",
    "                f.write(\"%s;\"%(vocab_list[L[i][j][k]]))\n",
    "            f.write(\"%s\\n\"%(vocab_list[L[i][j][len(L[i][j]) - 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
